[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "The Biology to be Explored in LLMs",
    "section": "",
    "text": "This post will be summarizing key points about molecular biology which I have learned from the article “Large Language Models in Molecular Biology” by Serafim Batzoglou.\nIn an attempt to further explore LLMs to prepare for my upcoming project regarding LLMs, the following summary of the biological information in the above article was generated by prompting ChatGPT and edited by me.\n\nCellular and Molecular Biology Components\n\nCentral Dogma\nThe central dogma of molecular biology explains how genetic information flows within living organisms. It states that DNA, which is housed in the nucleus of every cell, is the source of this genetic information. Human DNA consists of approximately 3 billion nucleotides organized into 23 chromosomes, with 22 being autosomes and one being a sex chromosome (X or Y). Each person inherits two nearly identical copies of the human genome, one from each parent. The genetic material from both parents is retained in the nucleus of each of the roughly 30 trillion cells in the human body. The genome contains about 20,000 genes responsible for protein synthesis, with only about 1% of the genome coding for proteins. The remaining portions of the genome include regions that control gene expression, regions within genes that do not code for proteins, regions contributing to DNA structure, and “junk” regions of self-replicating DNA.\nProtein synthesis, a fundamental process in molecular biology, involves three main steps: transcription, splicing, and translation. During transcription, a DNA segment serving as a gene template is copied into messenger RNA (mRNA). The mRNA molecule undergoes splicing, where certain segments, called introns, are removed, and the remaining segments, called exons, are joined together to form mature mRNA. Splicing is crucial in higher organisms because it allows a single gene to produce multiple protein variants by assembling different combinations of exons. The mRNA is then transported to the ribosome, where translation occurs. During translation, the mRNA sequence is decoded into amino acids, which are the building blocks of proteins. These amino acids are linked together to form a protein sequence, which folds into a functional three-dimensional structure. Proteins play essential roles in various biological processes, providing structural components, catalyzing reactions as enzymes, and facilitating communication and transportation within cells.\n\n\nGene Regulation\nGene regulation is a complex process that controls when, where, and in what quantity genes are expressed in cells. It ensures the timely production of the right proteins in appropriate amounts. Gene regulation occurs at different levels, involving chromatin structure, chemical modifications, and the action of transcription factors. Transcription factors are proteins that bind to specific DNA sequences and influence the recruitment of RNA polymerase, the enzyme responsible for mRNA synthesis. They help regulate the expression of target genes to ensure they are appropriately expressed in response to signals.\nPromoters and enhancers are DNA regions that contribute to gene expression control, with promoters located adjacent to gene starts and enhancers situated within introns or between genes, further downstream in the DNA. Chromatin structure, formed by DNA wrapping around histone proteins, determines which DNA regions are accessible for gene expression. Chemical modifications of histones and DNA, such as acetylation, methylation, and DNA methylation, can influence chromatin structure and gene expression. Gene regulation is specific to each type of cell - some cells have certain genes expressed while other cells have different genes expressed. This is what allows cells to have specialized functions.\nThe flow of genetic information is traditionally described as unidirectional: DNA to RNA to protein. However, there are exceptions to this rule. Reverse transcription allows RNA to be converted back into DNA, as seen in retroviruses like HIV. DNA can also be transcribed into different types of RNA, such as transfer RNA (tRNA) and ribosomal RNA (rRNA), adding complexity to genetic information flow.\n\n\nEpigenetic Mechanisms\nEpigenetic mechanisms, including DNA methylation and histone modifications, play a role in gene regulation and can be inherited. DNA methylation is a chemical modification where methyl is added to the DNA molecule, usually at specific cytosine bases. Methylation influences gene expression by affecting the binding of transcription factors and the chromatin structure. Chromatin must be unfolded for gene expression, so by making the chromatin more compact, methylation makes transcription more difficult (affects gene accessibility).\nDNA variation contributes to the diversity and heritability of traits among individuals. DNA variants are introduced primarily through mutations between the genomes of parents and germline genomes passed on to offspring. Deleterious variants tend to be eliminated from the population over time through natural selection. Genetic variations common in humans are typically benign or contribute to diseases that manifest later in life. Some rare mutations can affect the splicing sites (the boundaries where genes are spliced). As a result, they can cause the production of a completely different protein sequence, thus different protein function. This is why they contribute to 10% of rare genetic diseases.\nSo, predicting splice sites and determining gene structure is important to diagnose genetic diseases."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Basics of LLMs and Their Role in the Field of Biology",
    "section": "",
    "text": "Computational Components (LLMs)\n\nWhat is an LLM?\nA Large Language Model is a type of neural network which uses vast amounts of textual data in order to generate text composed of human language. By identifying patterns and context within the text which is inputted, it is able to respond to questions, create new content, and even make predictions.\n\n\nWhat are the different types of LLMs?\n\nWord grams: These are rudimentary models that predict the next word based on the frequency of word pairs or word bags in the training data. They DO NOT consider context or word order, resulting in less coherent predictions. Text generated using word grams often lacks resemblance to human text.\nCNNs (Convolutional Neural Networks): CNN models analyze text by considering relationships between adjacent words within a fixed window. They can have wide windows using techniques like dilation. While CNNs are good at identifying local patterns, they struggle with capturing long-range dependencies and comprehending complex sentence structures.\nLSTMs (Long Short-Term Memory networks): LSTMs are a variant of Recurrent Neural Networks (RNNs) capable of storing and processing information from earlier parts of a text. They outperform CNNs in understanding context and managing long-range dependencies. However, they still face challenges with complex sentences and long text.\nAttention Mechanisms: Attention mechanisms are not models in themselves, but mechanisms. They allow models to focus on relevant parts of the input when making predictions. These models have multiple attention “heads” that can concentrate on different parts of the previous text. Transformers, a class of language models, implement attention mechanisms.\nLarge Language Models (LLMs): LLMs, such as GPT-3, are transformers trained on vast amounts of data. Their large size facilitates the learning of intricate patterns, relationships, and context within the text. LLMs represent the most advanced language models available and can generate accurate and coherent responses across a wide range of topics.\n\nThe following LLMs use transformer architecture and were breakthroughs in the field:\n\nBERT (Bidirectional Encoder Representations from Transformers): BERT is a series of LLMs introduced by Google. It is trained using masked language modeling and next sentence prediction. BERT understands context from both the left and right sides of the input, making it bidirectional. It has been open-sourced and achieved significant advancements in language understanding.\nGPT (Generative Pretrained Transformer): GPT is a series of LLMs introduced by OpenAI. Unlike BERT, GPT is trained using the traditional language modeling task of autocompletion. It attends only to the left context during training, making it unidirectional. GPT excels in tasks involving text generation and has shown remarkable performance across various domains.\n\nThese types of LLMs vary in their modeling capabilities, with LSTMs and transformers like BERT and GPT being more advanced in understanding context and generating coherent responses. LLMs have significantly evolved, and the latest generation, such as GPT-4, exhibits promising signs of general intelligence.\n\n\nWhat about Data Generation for LLMs for Use in Genetics?\nAdvances in DNA sequencing have allowed us to fully sequence the entire human genome for less than $200. Sequencing-based methods have significantly advanced our ability to measure molecular function. These methods allow for the exposure of crucial molecular information such as chromatin structure, histone modifications, and transcription factor binding to DNA. Short DNA segments with specific properties of interest are isolated and sequenced in experiments to obtain this information. The rapid progress in DNA sequencing technology has outpaced Moore’s law and enabled the measurement of various genetic aspects within biological samples, including gene expression, chromatin accessibility, and histone modifications, often with single-cell or spatial precision.\n\n\nUsing LLMs for Diagnosing Genetic Diseases\nAs mentioned in an earlier post, mutations at splicing sites can completely change which proteins are produced, thus the protein function, resulting in rare genetic diseases. However, using LLMs, predicting splice sites and deducing gene structure becomes simpler and contribute to the diagnosis of rare genetic diseases.\nSpliceAI is a deep residual Convolutional Neural Network (CNN) introduced by the Illumina AI laboratory. It operates by utilizing earlier techniques for language modeling applied to DNA sequences, rather than functioning as a Language Model itself. Its primary purpose is to accurately predict the locations of intron-exon boundaries in the human genome, specifically the donor and acceptor sites. SpliceAI achieved a high precision-recall area under the curve (PR-AUC) score of 0.98, surpassing the previous best score of 0.23.\nOne key feature of SpliceAI is its ability to perform in silico mutational analysis. It can artificially modify DNA positions and determine whether the alterations introduce or eliminate splice sites within 10,000 nucleotides of the mutation. This capability makes SpliceAI valuable for aiding genetic diagnosis, particularly in cases of rare undiagnosed pediatric diseases. By inputting variants of a patient’s DNA into SpliceAI, it can assess the likelihood of altering gene splicing and disrupting gene function. SpliceAI’s high accuracy stems from its deep residual network’s capacity to learn complex biomolecular properties of DNA sequences that guide the splicing machinery to the correct splice sites. It captures and utilizes these previously unknown or imprecisely known properties effectively.\n\n\nPredicting Gene Expression from a DNA Sequence Using LLMs\n\nEnformer is a transformer-based tool and a part of the lineage of language models designed to predict cell type-specific gene expression levels based on the DNA sequence near a gene. It is trained using supervised learning to predict various experimental data types for a given genome region, including chromatin status, histone modifications, transcription factor binding, and gene expression levels. By incorporating attention mechanisms, Enformer can effectively capture correlations between molecular entities across distant regions up to 100,000 nucleotides away.\nWhile Enformer performs reasonably well in predicting gene expression from sequence alone, it currently falls short compared to experimental replicates, correlating at a level of 0.85, with a three-fold higher error rate. However, as more data are incorporated and the model is improved, its performance is expected to enhance. Enformer can also predict changes in gene expression caused by mutations in different individuals and artificially introduced mutations through CRISPR experiments. However, it has limitations in predicting the effects of distal enhancers and determining the direction of the impact of personal variants on gene expression, likely due to insufficient training data.\n\n\n\nEnformers for effective gene expression prediction. Credit: Erik Storrs blog.\n\n\n\n\nFoundation Models\nFoundation models, such as the transformer-based GPT models, are large deep learning architectures that encode a vast amount of knowledge from various sources. They can be fine-tuned for specific tasks, resulting in high-performance systems for different applications. Two recent preprint models in molecular biology are introduced: scGPT and Nucleotide Transformer.\nscGPT is designed for single-cell transcriptomics, chromatin accessibility, and protein abundance. It is trained on single-cell data from 10 million human cells and learns embeddings that provide insights into cellular states and biological pathways. The model is trained to generate data based on gene prompts and cell prompts, predicting genes and their confidence values. scGPT is then fine-tuned for tasks like batch correction, cell annotation, perturbation prediction, multiomics, and pathway prediction.\nNucleotide Transformer focuses on raw DNA sequences and uses the BERT methodology. It tokenizes sequences into k-mers of length 6 and is trained on the reference human genome, diverse human genomes, and genomes of other species. It is applied to 18 downstream tasks, including promoter prediction, splice site prediction, and histone modifications. Predictions are made through probing or computationally inexpensive fine-tuning.\n\n\nWhat the AI Actually Does: Training LLMs in Predicting Gene Expression\nTeach it one-step causality relationships: “if a certain mutation occurs, a specific gene malfunctions. If this gene is under-expressed, other genes in the cascade increase or decrease” (Batzoglou). Ultimately, we want it to learn the complex statistical properties of existing biological systems. Batzoglou states that it can be “learned from triangulating between correlations across modalities such as DNA variation, protein abundance and phenotype (a technique known as Mendelian randomization)”.\nIn all, the deep learning technology is strong enough at this point to take in genomic data and output predictions for gene expression or other biological information. These technologies are continuously being developed, becoming even more powerful, efficient, and precise day-by-day."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My ‘LLM in Biology’ Blog!",
    "section": "",
    "text": "This is the introductory post about the blog!\n\n\n\nCredit: Serafim Batzoglou, “Large Language Models in Biology”. Image from the author, created by Midjourney, prompted by “DNA”.\n\n\nThis blog will ultimately track my progress in learning about Large Language Models (LLMs) and their applications in molecular biology throughout the course of my 8-week program in the Im Lab at the University of Chicago. As I continue to learn biological knowledge and computational skills, I will continue to update this blog with what I have learned."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Large Language Models in Molecular Biology",
    "section": "",
    "text": "The Biology to be Explored in LLMs\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nBasics of LLMs and Their Role in the Field of Biology\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My ‘LLM in Biology’ Blog!\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is essentially a project of progress reports on biological knowledge and computational skills that I have acquired over my 8 weeks in the Im Lab during Summer 2023 for the GPT in Genomics Project."
  }
]